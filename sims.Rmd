---
title: "sims"
author: "Rich"
date: "`r Sys.Date()`"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This files runs simulations for the AI minimal group exps based on the
general workings in this folder: /exps/R/simulations/brms/planning/, as well as
here: /exps/eliane/congruency/paintings/simulation_ml/

The basic structure and code follows the examples outlined by Solomon Kurz in his
'power' blogs and Lisa Debruine's 'faux' package. 

For Solomon's blog posts, see here: https://solomonkurz.netlify.app/tags/power/

For Lisa's faux package, see here: https://debruine.github.io/faux/


The basic workflow is as follows:
1. Simulate one dataset.
2. Build one model in brms with justifiable priors set.
3. Create a function to loop through 1:n_sims and each time update the model
created in step (2) using the brms function 'update'. This saves time compiling
the model each time, which is important if we are going to build 500/1000 sims.
Note: the brms::update function is only available for simpler models, AFIK.

4. Then look at widths instead of "power". 

The development file for this code is saved as sims_testing.Rmd.

This file is a streamlined version, which only keeps the key aspects.


## install packages ##

```{r install-pkg}
# install.packages("remotes")
# remotes::install_github("stan-dev/cmdstanr")
# install.packages(c("brms", "tidyverse", "future", "faux")) # parallel comes pre-installed, hence it only needs loading in the next chunk and not installing.
# 
# # this helps to visualise priors
# install.packages("devtools")
# devtools::install_github("jmgirard/standist")
```

## load ##

```{r load-pkg}
pkg <- c("cmdstanr", "brms", "tidyverse", "future", "parallel", "faux", "standist")

lapply(pkg, library, character.only = TRUE)
```

## settings ##

```{r set-options}
options(brms.backend = "cmdstanr",
        mc.cores = parallel::detectCores(),
        future.fork.enable = TRUE,
        future.rng.onMisuse = "ignore") ## automatically set in RStudio

supportsMulticore()

detectCores()
```

## snapshot ##

```{r snapshot-renv}
# take a snapshot and update the lock.file
# renv::snapshot() # this is only necessary when new packages or installed or packages are updated.
```

## 1) create some initial data ##

As part of the sims_testng.Rmd file, we first fit a model to prior data from a past experiment, which
had a similar structure and design so that it could help us guide our choices of SDs and cor values 
below when creating the data.

for details on the model - bd5.1cs - which is from the minimal group "dots" experiment, but with the 
DV in centred and standardised units, load the model as follows...

```{r}
# past model with a factorial design with the DV in centred and standardised units
bd5.1cs <- readRDS("models/bd5.1cs")
summary(bd5.1cs)

# # the regular model in original units is 
# bd5.1 <- readRDS("models/bd5.1.rds")
# # and the index coding model is
# bi2 <- readRDS("models/bi2.rds")
```

create the data

to allow more datapoints per pid and help with estimating varying effects in the model,
I coded the normal and flipped presentation of the matrices as item_n = 1,2. This just means
we supply 6 datapoints per pid rather than averaging across item_n first.

```{r}
subj_n = 500  # number of subjects
item_n = 2  # number of items (normal and flipped versions)

# varying effects 
u0s_sd = 0.6  # varying intercept SD for subjects
u0i_sd = 0.1  # varying intercept SD for items - we expect this to be v. small
u1s_sd = 0.7  # varying b1 slope SD for subjects (effect of mat2v1)
u2s_sd = 0.2  # varying b2 slope SD for subjects (effect of mat3v1)
r01s = -0.7   # correlation between varying effects 0 and 1 for subjects
r02s = 0.2   # correlation between varying effects 0 and 2 for subjects
r12s = -0.1   # correlation between varying effects 1 and 2 for subjects

# fixed effects in standardised units?
b0 = 0      # intercept
b1 = -0.4     # fixed effect of matrix type m2v1
b2 = 0.2    # fixed effect of matrix type m3v1
b3 = 0.2    # fixed effect of agent

# sigma
sigma_sd = 0.4 # error SD

set.seed(1)

d <- add_random(subj = subj_n, item = item_n) %>%
  add_between(.by = "subj", 
              agent = c("human", "AI")) %>%
  add_contrast("agent", "anova", add_cols = TRUE, colnames = "agentd") %>%
  add_within(.by = "item",
             matrix_type = c("mat1", "mat2", "mat3")) %>% 
  add_contrast("matrix_type", "anova", add_cols = TRUE, 
               colnames = c("mat2v1", "mat3v1")) %>%
  add_ranef("subj", u0s = u0s_sd, u1s = u1s_sd, u2s = u2s_sd, 
            .cors = c(r01s, r02s, r12s)) %>%
  add_ranef("item", u0i = u0i_sd) %>%
  add_ranef(sigma = sigma_sd) %>%
  # calculate DV
  mutate(y = b0 + u0s + u0i + (b1 + u1s) * mat2v1 + (b2 + u2s) * mat3v1 
         + b3 * agentd + sigma)
head(d)
```

plot

```{r}
ggplot(d, aes(matrix_type, y, color = agent)) +
  geom_hline(yintercept = b0) +
  geom_hline(yintercept = 0, colour = "RED") +
  geom_violin(alpha = 0.5) +
  stat_summary(fun = mean,
               fun.min = \(x){mean(x) - sd(x)/sqrt(250)},
               fun.max = \(x){mean(x) + sd(x)/sqrt(250)},
               position = position_dodge(width = 0.9)) +
  scale_color_brewer(palette = "Dark2")
```

```{r}
ggplot(d, aes(x=y, fill=agent)) +
   geom_density(alpha = 0.3, colour = "darkgrey") +
   scale_fill_brewer(palette = "Dark2")+
   theme(panel.grid = element_blank()) +
   theme(legend.position = "none") +
   ggtitle("response by condition") +
   facet_wrap(~matrix_type, ncol = 1)
```


## 2) fit an initial model ##

2x3 factorial design, but without interactions, given that we are not simulating to estimate
the size / power / precision of interactive effects. In the real thing, we will calculate
interaction terms, but we will not have a prior precision estimate because our pre-registered
hypothesis is focussed on assessing the size and direction of the effect of agent.

formula

```{r}
formula = bf(y ~ 1 + agentd + mat2v1 + mat3v1 +
               (1 + mat2v1 + mat3v1 | subj) +
               (1 | item))
```

check priors

```{r}
get_prior(data = d,
          family = gaussian,
          formula)
```

visualise prior settings

maybe we could set narrower priors that we would, given that we know the ground truth, which
means we can check if the parameters recovered are sensible. 

and then in the real thing we can set more weakly informative priors??

Yes, this sounds good because it saves a lot of computational time, which really matters
when we are fitting 1000 models per variation in simulated data.

```{r}
visualize("normal(0, 1)", "normal(0, 0.5)", "normal(0, 0.25)", "normal(0, 0.1)", 
          xlim = c(-4, 4))
```

set priors

weakly informative priors might look like this in this context

```{r}
# priors <- c(
#   set_prior("normal(0, 1)", class = "Intercept"),
#   set_prior("normal(0, 0.5)", class = "b"),
#   set_prior("normal(0, 0.5)", class = "sigma"),
#   set_prior('normal(0, 0.5)', class = 'sd'),
#   set_prior("lkj(2)", class = "cor") # correlation between varying effects log-units
# )
```

narrower priors might look like this. Let's use these.

```{r}
priors <- c(
  set_prior("normal(0, 0.1)", class = "Intercept"),
  set_prior("normal(0, 0.1)", class = "b"),
  set_prior("normal(0, 0.1)", class = "sigma"),
  set_prior('normal(0, 0.1)', class = 'sd'),
  set_prior("lkj(2)", class = "cor") # correlation between varying effects log-units
)
```

fit the model

with narrow priors (narrower than we would usually choose, at least)

```{r}
t1 <- Sys.time()

plan(multicore)
fit <-
  brm(data = d,
      family = gaussian,
      formula = formula,
      prior = priors,
      iter = 2000, warmup = 1000,
      cores = 20, 
      control = list(adapt_delta = 0.95),
      save_pars = save_pars(all=TRUE),
      seed = 1)

t2 <- Sys.time()

t2 - t1

# Time difference of 1.638154 mins imac
```

let's take a look 

```{r}
# chains
plot(fit)
# summary
print(fit)
# fixed effects
fixef(fit)

# save initial fit
saveRDS(fit, "models/fit.rds")
```


## 3) update the fit ##

update the fit and check the time taken (this is unnecessary, but just to see 
the time difference, which is usually quite large).

This uses the model with narrow priors

```{r}
set.seed(2)

# create new data
d2 <- add_random(subj = subj_n, item = item_n) %>%
  add_between(.by = "subj", 
              agent = c("human", "AI")) %>%
  add_contrast("agent", "anova", add_cols = TRUE, colnames = "agentd") %>%
  add_within(.by = "item",
             matrix_type = c("mat1", "mat2", "mat3")) %>% 
  add_contrast("matrix_type", "anova", add_cols = TRUE, 
               colnames = c("mat2v1", "mat3v1")) %>%
  add_ranef("subj", u0s = u0s_sd, u1s = u1s_sd, u2s = u2s_sd, 
            .cors = c(r01s, r02s, r12s)) %>%
  add_ranef("item", u0i = u0i_sd) %>%
  add_ranef(sigma = sigma_sd) %>%
  # calculate DV
  mutate(y = b0 + u0s + u0i + (b1 + u1s) * mat2v1 + (b2 + u2s) * mat3v1 
         + b3 * agentd + sigma)
head(d2)

# update the fit and supply new data

t1 <- Sys.time()

updated_fit <-
  update(fit,
         newdata = d2,
         seed = 2)

t2 <- Sys.time()

t2 - t1

# Time difference of 1.304859 mins imac
```

let's take a look 

```{r}
# chains
plot(updated_fit)
# summary
print(updated_fit)
# fixed effects
fixef(updated_fit)

# save the updated fit
# saveRDS(fit, "models/updated_fit.rds")
```


## 4) create a function to simulate multiple datasets and build models ##

create a function called sim

```{r}
sim <- function(subj_n = 500, item_n = 2,  # these can be changed when calling the function
                b0 = 0, b1 = -0.4, b2 = 0.2, b3 = 0.2,      # fixed effects 
                u0s_sd = 0.6, u0i_sd = 0.1,  # random intercepts pid and items
                u1s_sd = 0.7, u2s_sd = 0.2, # random slope
                r01s = -0.7, r02s = 0.2, r12s = -0.1,   # cors
                sigma_sd = 0.4,           # error term
                ... # helps the function work with pmap() below
                ) {

  # set up data structure
  data <- add_random(subj = subj_n, item = item_n) %>%
    add_between(.by = "subj", 
                agent = c("human", "AI")) %>%
    add_contrast("agent", "anova", add_cols = TRUE, colnames = "agentd") %>%
    add_within(matrix_type = c("mat1", "mat2", "mat3")) %>% 
    add_contrast("matrix_type", "anova", add_cols = TRUE, 
                 colnames = c("mat2v1", "mat3v1")) %>%
    add_ranef("subj", u0s = u0s_sd, u1s = u1s_sd, u2s = u2s_sd, 
              .cors = c(r01s, r02s, r12s)) %>%
    add_ranef("item", u0i = u0i_sd) %>%
    add_ranef(sigma = sigma_sd) %>%
    # calculate DV
    mutate(y = b0 + u0s + u0i + (b1 + u1s) * mat2v1 + (b2 + u2s) * mat3v1 
         + b3 * agentd + sigma)
  
    # glimpse(data) # only use this when testing the code
}
```

Here’s a quick example of how our function works.

```{r}
sim(subj_n = 20, b0 = 0, b3 = 0.2) # if you uncomment glimpse above,
# it will let you glimpse the data that's generated. this is useful for checking / testing code purposes.
```


## 5) run the simulation ##

let's scale it up to multiple reps and only save the parameters of interest rather than the full brms model and the data.

Note 1: the advantage of not saving the brms model and data is that it saves memory, which can easily be exhausted with 1000s of sims.
The disadvantage is that you data and models cannot be explored/summarised afterwards because they are not kept.

Note 2: Depending on the size of the data and model, plus the number of sims, as well as the power of your machine, this will take some 
time. e.g., my 20 core imac takes 1.304859 mins per model (as estimated above). That means 1.3 mins * 1000 models = ~ 22 hours

```{r}
plan(multicore)
x <- crossing(
  rep = 1:1000, # number of replicates
  subj_n = 500, # range of subject N
  b3 = 0.2, # effect of agent 
) %>%
  mutate(d = pmap(., sim)) %>%
  mutate(params = map2(d, rep, ~update(fit, newdata = .x, seed = .y) %>% # if you left the code here, then it would store the models and data
                     fixef() %>% 
                     data.frame() %>% 
                     rownames_to_column("parameter"))) %>% 
  select(-d) # adding this line in removes the data from the stored tibble 'x'

```


## 6) take a look at the output ##

let's take a look

```{r}
head(x)
```

ok, this looks good. 

select parameters of interest to summarise and visualise

```{r}
parameters <-
  x %>% 
  unnest(params)
head(parameters)
```

save out parameters

```{r}
# save the parameters
write_csv(parameters, "data/sim_p.csv")
```


## 7) calculate power i.e., % Q2.5 > 0 ##

```{r}
power <- parameters %>% 
  filter(parameter == "agentd") %>%
  group_by(subj_n) %>% 
  mutate(check = ifelse(Q2.5 > 0, 1, 0)) %>% 
  summarise(power = mean(check)) %>% 
  mutate(subj_n = factor(subj_n))
power
```

plot power

```{r}
p_power <- ggplot(power, aes(x = subj_n, y = power, fill = power)) +
  geom_tile() +
  geom_text(aes(label = sprintf("%.2f", power)), color = "white", size = 10) +
  scale_fill_viridis_c(limits = c(0, 1)) 
p_power

ggsave ("figures/power.jpeg")
```

plot parameters and include power as a text label

wrangle

```{r}
plot_params <- parameters %>%
  filter(parameter == "agentd") %>%
  mutate(below_zero = if_else(Q2.5 < 0, "yes", "no"), 
         below_zero = factor(below_zero, levels = c("no", "yes")),
         subj_n = factor(subj_n)) %>% 
  inner_join(power, by = c("subj_n")) %>% 
  mutate(power = round(power * 100, 0)) 
  # mutate(power = if_else(power == 100, 99, power)) # avoid power = 100% when it isn't due to rounding
head(plot_params)
```

plot

the one that works so far (but with only 2 hlines and b1 is fixed at 0.5, which isn't right)

```{r}
p_params <- plot_params %>%
  ggplot(aes(x = rep, y = Estimate, ymin = Q2.5, ymax = Q97.5)) +
  geom_pointrange(fatten = 1/2, aes(colour=below_zero)) +
  geom_hline(yintercept = b0, colour = "red") +
  geom_hline(aes(yintercept = b3), colour = "blue") + # this would add a line at b1 - the target effect size
  scale_colour_manual(values=c("darkgrey","black")) +
  geom_text(aes(x=1.5, y=-0.1,
                label = sprintf("%.f%s", power, "% power")), color = "darkgrey", size = 4) +
  theme_bw() +
  theme(legend.position = "none") +
  labs(x = "sim # (i.e., simulation index)",
       y = expression(beta[3])) 
p_params

ggsave ("figures/parameters.jpeg")
```



## 8) let's now consider widths and precision, rather than NHST and power ##

## load in the saved parameters if necessary ##

This has 1000 sims. What's nice about this approach is that without sweating blood to think about what widths
to come up with, we can just simulate a range of reasonable/feasible sample sizes and effect sizes. We already know a lot about
both of these, given the practical and financial constraints of the type of data collections and effect sizes in psychology,
which tend to be small. So it might be a good way to go initially to get a sense.

ok, so let's look at widths when we have N=500. We ended up only simulating one sample size. N=500. Let's see what that gives us.

```{r}
sim_p <- read_csv("data/sim_p.csv") %>% 
  filter(parameter == "agentd") 
head(sim_p)

# if sim_p is already loaded then use this code
# sim_p <- sim_p %>% 
#   filter(parameter == "agentd") 
# head(sim_p)
```

Instead of just ordering the point-ranges by their seed values, we might instead arrange them by the lower levels.

```{r}
# wrangle to order by Q2.5
plot_p <- sim_p %>%
  arrange(subj_n, Q2.5) %>%
  mutate(rank = 1:1000) %>% # 1000 models per variation (1 N so far i.e., 500)
  mutate(subj_n = factor(subj_n))
head(plot_p)
str(plot_p)

# plot
p_params <- plot_p %>%
  ggplot(aes(x = rank, y = Estimate, ymin = Q2.5, ymax = Q97.5)) +
  geom_pointrange(fatten = 1/2) +
  geom_hline(aes(yintercept = b0), colour = "red") +
  theme_bw() +
  theme(legend.position = "none") +
  scale_x_discrete("reordered by the lower level of the 95% intervals", breaks = NULL) +
  ylab(expression(beta[3])) +
  facet_wrap(~subj_n)
p_params

ggsave("figures/params_by_Q2.5.jpeg")
```

Notice how this arrangement highlights the differences in widths among the intervals. The wider the interval, the less precise the estimate. Some intervals were wider than others, but all tended to hover in a similar range. We might quantify those ranges by computing a width variable.

```{r}
plot_p <-
  plot_p %>% 
  mutate(width = Q97.5 - Q2.5)

head(plot_p)
```

Here’s the width distribution.

```{r}
p_hist <- plot_p %>% 
  ggplot(aes(x = width, fill = subj_n, colour = subj_n)) +
  geom_histogram(binwidth = .01) +
  geom_rug(linewidth = 1/6) +
  theme_bw() +
  facet_wrap(~subj_n)
p_hist

ggsave("figures/width_hist.jpeg")

p_hist_2 <- plot_p %>%
  ggplot(aes(x = width, fill = subj_n, colour = subj_n)) +
  geom_histogram(binwidth = .01, position = 'identity', alpha = 0.7) +
  geom_rug(linewidth = 1/6) +
  scale_fill_discrete(breaks=c('500', '600')) +
  scale_colour_discrete(breaks="none") +
  theme_bw() +
  theme(legend.position = "right") + 
  # scale_x_continuous(breaks = seq(0.1, 0.3, 0.1), limits = c(0.1, 0.3)) +
  ggtitle("95% interval widths by simulated sample size")
p_hist_2

ggsave("figures/width_hist_2.jpeg")
```

The widths of our 95% intervals range from 0.16 to ~0.20 across the variation in sample size. 
Let’s focus a bit and take a random sample from a few of the simulation iterations.

So far, N=250 per group looks good, in short. Avg width about 0.19, biggest widths about 0.2.


Take a random sample to look at a few...

```{r}
set.seed(1)

plot_p %>%
  group_by(subj_n) %>% 
  sample_n(1) %>% 
  # mutate(rep = rep %>% as.character()) %>%

  ggplot(aes(x = Estimate, xmin = Q2.5, xmax = Q97.5, y = subj_n,
             colour = subj_n)) +
  geom_vline(xintercept = c(0, .5), color = "white") +
  geom_pointrange() +
  labs(x = expression(beta[3]),
       y = "subj #") +
  scale_y_discrete(breaks = c(500, 600)) +
  scale_x_continuous(breaks = seq(-0.4, 0.4, 0.2), limits = c(-0.4, 0.4)) 
```

So, instead of focusing on rejecting a null hypothesis, we might instead use our simulation skills to determine the sample size we need to have most of our 95% intervals come in at a certain level of precision. This has been termed the accuracy in parameter estimation [AIPE; Maxwell et al. ( 2008); see also Kruschke ( 2015)] approach to sample size planning.

Thinking in terms of AIPE, in terms of precision, let’s say we wanted widths of 0.3, 0.2, 0.1 or smaller. Here’s how we did with our sims.

```{r}
plot_p %>%
  group_by(subj_n) %>% 
  mutate(below_03 = if_else(width < .3, 1, 0),
         below_02 = if_else(width < .2, 1, 0),
         below_01 = if_else(width < .1, 1, 0)) %>% 
  summarise(power_03 = mean(below_03),
            power_02 = mean(below_02),
            power_01 = mean(below_01))

# subj_n power_03 power_02 power_01
#   <fct>     <dbl>    <dbl>    <dbl>
# 1 500           1        1        0
# 2 600           1        1        0
```

ok, at N=250 (or higher), widths < .3 and .2 look good. .1 looks terrible.

Our simulation suggests we have about a high probability of achieving 95% CI widths of 0.2 or smaller with n=250 or higher.

That last bit about excluding zero brings up an important point. Once we’re concerned about width size, about precision, the null hypothesis is no longer of direct relevance. And since we’re no longer wed to thinking in terms of the null hypothesis, there’s no real need to stick with a .8 threshold for evaluating width power (okay, I’ll stop using that term). Now if we wanted to stick with .8, we could. Though a little nonsensical, the .8 criterion would give our AIPE analyses a sense of familiarity with traditional power analyses, which some reviewers might appreciate. But in his text, Kruschke mentioned several other alternatives. One would be to set maximum value for our CI widths and simulate to find the nn necessary so all our simulations pass that criterion. Another would follow Joseph, Wolfson, and du Berger ( 1995, 1995), who suggested we shoot for an N that produces widths that pass that criterion on average. Here’s how we did based on the average-width criterion.

```{r}
plot_p %>%
  group_by(subj_n) %>%
  summarise(avg_width = mean(width))

# subj_n avg_width
#   <fct>      <dbl>
# 1 500        0.193
# 2 600        0.169
```

ok, so the average for 500 and 600 are pretty good? Below 0.2.
That would mean that an effect of 0.2 could be distinguished consistently from 
zero and nearly from 0.1...

And if the effect is larger, then we would equally be more confident about distinguishing it
from other effect sizes e.g.,

0.4 - we would be fairly confident that 0.3 and 0.5 are unlikely, across to our 95% interval.

So fairly precise?


